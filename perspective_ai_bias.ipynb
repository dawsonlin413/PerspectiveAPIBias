{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4GdkgAO-e6a"
      },
      "source": [
        "# PERSPECTIVE API BIAS TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MM2LVBNhsOXv"
      },
      "outputs": [],
      "source": [
        "from googleapiclient import discovery\n",
        "import json\n",
        "\n",
        "API_KEY = 'AIzaSyCdY34KLoLgJgSlghufQvtQl6hU8dcuxnI'\n",
        "\n",
        "def analyze_toxicity(comment_text):\n",
        "\n",
        "    client = discovery.build(\n",
        "        \"commentanalyzer\",\n",
        "        \"v1alpha1\",\n",
        "        developerKey=API_KEY,\n",
        "        discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
        "        static_discovery=False,\n",
        "    )\n",
        "\n",
        "    analyze_request = {\n",
        "        'comment': {'text': comment_text},\n",
        "        'requestedAttributes': {'TOXICITY': {}}\n",
        "    }\n",
        "\n",
        "    response = client.comments().analyze(body=analyze_request).execute()\n",
        "    toxicity_score = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
        "    return toxicity_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb2wbd7y1cLK"
      },
      "source": [
        "Imported libraries and created the function for the Perspective API which returns toxicity score based on the comments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2wyMppUF2Kr"
      },
      "source": [
        "# TEST 1: LONG VS SHORT COMMENTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vxUnLYPHkdq"
      },
      "source": [
        "### HYPOTHESIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oec1p8ocHqZN"
      },
      "source": [
        "I hypothesize that the Perspective API will detect higher toxicity scores in longer comments compared to lowercase comments, despite their content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qxF8Ey0kB5Ru"
      },
      "outputs": [],
      "source": [
        "long_comments = [\n",
        "    \"This is a long comment that is longer than 100 characters. It should be truncated.\",\n",
        "    \"I disagree with this comment. It's too long.\",\n",
        "    \"I'm really excited about this project. It's a lot of fun.\",\n",
        "    \"I love this product. It's so good!\",\n",
        "    \"I'm frustrated with this comment. It's so long.\",\n",
        "    \"I'm frustrated with this comment. It's really long.\",\n",
        "    \"I'm frustrated with this comment. It's way too long.\",\n",
        "]\n",
        "short_comments = [\n",
        "    \"Hey\",\n",
        "    \"Good morning\",\n",
        "    \"Good afternoon\",\n",
        "    \"Good evening\",\n",
        "    \"Good night\",\n",
        "    \"Goodbye\",\n",
        "    \"See you later\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create comments only based on the length of the sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QBKozhwjGWCX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uppercase Comments Results: [0.050326355, 0.030977672, 0.022964042, 0.020842785, 0.07371122, 0.07371122, 0.069754265]\n",
            "Lowercase Comments Results: [0.020842785, 0.019728716, 0.02035702, 0.02035702, 0.022374803, 0.02449606, 0.038519915]\n"
          ]
        }
      ],
      "source": [
        "long_toxicity = [analyze_toxicity(comment) for comment in long_comments]\n",
        "short_toxicity = [analyze_toxicity(comment) for comment in short_comments]\n",
        "\n",
        "print(\"Uppercase Comments Results:\", long_toxicity)\n",
        "print(\"Lowercase Comments Results:\", short_toxicity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vffe1ZMOwOkv"
      },
      "source": [
        "Store and print the toxicity results for the long and short comments toxicity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GmuFsSh9xSgW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Long Comments Average Toxicity: 0.04889822271428571\n",
            "Short Comments Average Toxicity: 0.023810902714285716\n",
            "Longer comments have a higher toxicity.\n"
          ]
        }
      ],
      "source": [
        "average_long = sum(long_toxicity) / len(long_toxicity)\n",
        "average_short = sum(short_toxicity) / len(short_toxicity)\n",
        "\n",
        "print(\"Long Comments Average Toxicity:\", average_long)\n",
        "print(\"Short Comments Average Toxicity:\", average_short)\n",
        "\n",
        "if average_long > average_short:\n",
        "    print(\"Longer comments have a higher toxicity.\")\n",
        "elif average_long < average_short:\n",
        "    print(\"Shorter comments have a higher toxicity.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0GzHcTMxh_w"
      },
      "source": [
        "Use the average to easily calculate which type of comments are seen as more toxic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1C-tl7q0nxy"
      },
      "source": [
        "### CONCLUSION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ugGjHZa0vv2"
      },
      "source": [
        "My hypothesis was correct in that longer comments have a higher toxicity than shorter comments. (0.049>0.024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTRLoeFgGZer"
      },
      "source": [
        "# TEST 2: MALE VS FEMALE GENDERED COMMENTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmw2KnuoIsGa"
      },
      "source": [
        "### HYPOTHESIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTEUWizgI1zF"
      },
      "source": [
        "I hypothesize that the Perspective API will detect higher toxicity scores in female-gendered comments compared to male-gendered comments, despite their content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wf2iuZC3vDc"
      },
      "source": [
        "### INITIALIZE COMMENT DATA STORAGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Zylw21ZVCX_X"
      },
      "outputs": [],
      "source": [
        "male_gendered = [\n",
        "    \"Men are useless\",\n",
        "    \"Men can't drive\",\n",
        "    \"Men are stupid\",\n",
        "    \"Men are the worst\",\n",
        "    \"Men are mean\",\n",
        "    \"Men are fat\",\n",
        "    \"Men are ugly\"\n",
        "]\n",
        "female_gendered = [\n",
        "    \"Women are useless\",\n",
        "    \"Women can't drive\",\n",
        "    \"Women are stupid\",\n",
        "    \"Women are the worst\",\n",
        "    \"Women are mean\",\n",
        "    \"Women are fat\",\n",
        "    \"Women are ugly\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hUeeyZH3ogI"
      },
      "source": [
        "Create comments only differing by the gender."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Male Gendered Comments Results: [0.6407703, 0.35186127, 0.88599813, 0.52811706, 0.37795907, 0.51869136, 0.6588125]\n",
            "Female Gendered Comments Results: [0.7570315, 0.4826145, 0.91625386, 0.6544696, 0.5716857, 0.60311776, 0.7675452]\n"
          ]
        }
      ],
      "source": [
        "male_toxicity = [analyze_toxicity(comment) for comment in male_gendered]\n",
        "female_toxicity = [analyze_toxicity(comment) for comment in female_gendered]\n",
        "\n",
        "print(\"Male Gendered Comments Results:\", male_toxicity)\n",
        "print(\"Female Gendered Comments Results:\", female_toxicity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Store and print the toxicity results for the long and short comments toxicity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Male Gendered Comments Average Toxicity: 0.5660299557142857\n",
            "Female Gendered Comments Average Toxicity: 0.6789597314285715\n",
            "Female gendered comments have a higher toxicity.\n"
          ]
        }
      ],
      "source": [
        "average_male = sum(male_toxicity) / len(male_toxicity)\n",
        "average_female = sum(female_toxicity) / len(female_toxicity)\n",
        "\n",
        "print(\"Male Gendered Comments Average Toxicity:\", average_male)\n",
        "print(\"Female Gendered Comments Average Toxicity:\", average_female)\n",
        "\n",
        "if average_male > average_female:\n",
        "    print(\"Male gendered comments have a higher toxicity.\")\n",
        "elif average_male < average_female:\n",
        "    print(\"Female gendered comments have a higher toxicity.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the average to easily calculate which type of comments are seen as more toxic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CONCLUSION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "My hypothesis was correct in that female gendered comments have a higher toxicity than male gendered comments. (0.68>0.57)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
